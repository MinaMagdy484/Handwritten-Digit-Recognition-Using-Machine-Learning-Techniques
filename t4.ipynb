{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, mean_squared_error, r2_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = r'D:\\Fourth Year\\First Term\\Machine\\Lectures\\project\\10000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    \"\"\"Enhanced image preprocessing\"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray_img = img.convert('L')\n",
    "    \n",
    "    # Resize\n",
    "    small_img = gray_img.resize((28, 28))\n",
    "    \n",
    "    # Convert to numpy array and normalize\n",
    "    img_array = np.array(small_img).flatten() / 255.0\n",
    "    \n",
    "    # Apply contrast enhancement\n",
    "    img_array = np.clip((img_array - img_array.mean()) * 1.5 + 0.5, 0, 1)\n",
    "    \n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = []\n",
    "img_labels = []\n",
    "processed_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking directory structure...\n",
      "Found 10 class folders: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking directory structure...\")\n",
    "if not os.path.exists(IMG_DIR):\n",
    "    raise ValueError(f\"Directory {IMG_DIR} does not exist!\")\n",
    "\n",
    "class_folders = [f for f in os.listdir(IMG_DIR) if os.path.isdir(os.path.join(IMG_DIR, f))]\n",
    "if not class_folders:\n",
    "    raise ValueError(f\"No subdirectories found in {IMG_DIR}\")\n",
    "\n",
    "print(f\"Found {len(class_folders)} class folders: {class_folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 0 (label 0)\n",
      "Found 1000 images in 0\n",
      "Successfully processed 1000 images for class 0\n",
      "\n",
      "Processing class 1 (label 1)\n",
      "Found 1000 images in 1\n",
      "Successfully processed 1000 images for class 1\n",
      "\n",
      "Processing class 2 (label 2)\n",
      "Found 1000 images in 2\n",
      "Successfully processed 1000 images for class 2\n",
      "\n",
      "Processing class 3 (label 3)\n",
      "Found 1000 images in 3\n",
      "Successfully processed 1000 images for class 3\n",
      "\n",
      "Processing class 4 (label 4)\n",
      "Found 1000 images in 4\n",
      "Successfully processed 1000 images for class 4\n",
      "\n",
      "Processing class 5 (label 5)\n",
      "Found 1000 images in 5\n",
      "Successfully processed 1000 images for class 5\n",
      "\n",
      "Processing class 6 (label 6)\n",
      "Found 1000 images in 6\n",
      "Successfully processed 1000 images for class 6\n",
      "\n",
      "Processing class 7 (label 7)\n",
      "Found 1000 images in 7\n",
      "Successfully processed 1000 images for class 7\n",
      "\n",
      "Processing class 8 (label 8)\n",
      "Found 1000 images in 8\n",
      "Successfully processed 1000 images for class 8\n",
      "\n",
      "Processing class 9 (label 9)\n",
      "Found 1000 images in 9\n",
      "Successfully processed 1000 images for class 9\n"
     ]
    }
   ],
   "source": [
    "for label_idx, folder in enumerate(class_folders):\n",
    "    folder_path = os.path.join(IMG_DIR, folder)\n",
    "    files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    print(f\"\\nProcessing class {folder} (label {label_idx})\")\n",
    "    print(f\"Found {len(files)} images in {folder}\")\n",
    "    \n",
    "    for img_file in files:\n",
    "        try:\n",
    "            full_path = os.path.join(folder_path, img_file)\n",
    "            with Image.open(full_path) as img:\n",
    "                processed_img = preprocess_image(img)\n",
    "                processed_data.append(processed_img)\n",
    "                img_paths.append(full_path)\n",
    "                img_labels.append(label_idx)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    class_count = sum(1 for label in img_labels if label == label_idx)\n",
    "    print(f\"Successfully processed {class_count} images for class {label_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(processed_data)\n",
    "y = np.array(img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.DataFrame(X_scaled)\n",
    "image_df['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = image_df.drop(columns=['target'])\n",
    "labels = image_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_performance(model, train_X, train_y, test_X, test_y, model_name=\"\"):\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(test_X)\n",
    "    \n",
    "    if isinstance(model, LinearRegression):\n",
    "        mse = mean_squared_error(test_y, preds)\n",
    "        r2 = r2_score(test_y, preds)\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RÂ² Score: {r2:.4f}\")\n",
    "    else:\n",
    "        acc = accuracy_score(test_y, preds)\n",
    "        prec = precision_score(test_y, preds, average='weighted', zero_division=0)\n",
    "        rec = recall_score(test_y, preds, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(test_y, preds, average='weighted', zero_division=0)\n",
    "        conf_mat = confusion_matrix(test_y, preds)\n",
    "        \n",
    "        print(f\"Accuracy: {acc:.4f}\")\n",
    "        print(f\"Precision: {prec:.4f}\")\n",
    "        print(f\"Recall: {rec:.4f}\")\n",
    "        print(f\"F1: {f1:.4f}\")\n",
    "        if hasattr(model, 'penalty'):\n",
    "            print(f\"Regularization type: {model.penalty}\")\n",
    "            print(f\"Regularization strength (C): {model.C}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(conf_mat)\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        penalty='l2',  # L2 regularization\n",
    "        C=0.1,        # Stronger regularization\n",
    "        solver='lbfgs',\n",
    "        multi_class='multinomial',\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Gaussian NB\": GaussianNB()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Logistic Regression hyperparameters...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTuning Logistic Regression hyperparameters...\")\n",
    "param_dist = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10.0],       # Regularization strength\n",
    "    'penalty': ['l1', 'l2'],                   # Regularization type\n",
    "    'solver': ['liblinear', 'saga'],           # Solvers that support both l1 and l2\n",
    "    'multi_class': ['ovr'],\n",
    "    'max_iter': [1000],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(\n",
    "    LogisticRegression(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Use 10 random combinations\n",
    "    cv=5,       # Use 3-fold cross-validation for faster computation\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Fourth Year\\First Term\\Machine\\Lectures\\project\\p2\\t4.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Fourth%20Year/First%20Term/Machine/Lectures/project/p2/t4.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBest LogReg params found: \u001b[39m\u001b[39m{\u001b[39;00mgrid\u001b[39m.\u001b[39mbest_params_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Fourth%20Year/First%20Term/Machine/Lectures/project/p2/t4.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_log_reg \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mbest_estimator_\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "print(f\"\\nBest LogReg params found: {grid.best_params_}\")\n",
    "best_log_reg = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
